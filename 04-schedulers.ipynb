{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "## Threads, Processes, and the Global Interpreter Lock\n",
    "\n",
    "There are two main ways to achieve single-machine parallelism in python: multiple threads or multiple processes.\n",
    "The best choice depends on the type of work you're doing and Python's GIL (global interpreter lock).\n",
    "\n",
    "The GIL is an implementation detail of CPython. It's a lock deep inside your python process that limits things so *only one thread in your process can be running python code at once*. Let's see an example, using `concurrent.futures` to compute the Fibonnaci numbers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def fib(n):\n",
    "    \"\"\"Compute the `n`th fibonnaci number.\n",
    "    \n",
    "    This is a deliberatly slow, CPU intensive implemenation.\n",
    "    \"\"\"\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fib(n - 2) + fib(n - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time fib(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One `fib(34)` takes my machine about 2 seconds. Computing it 4 time should then take about 8 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = map(fib, [34, 34, 34, 34])\n",
    "%time _ = list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute it in parallel. This is embarrassingly parallel, so with 4 threads we *should* be back down to 2 seconds again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=4) as pool:\n",
    "    results = thread_pool.map(fib, [34, 34, 34, 34])\n",
    "    %time _ = list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's actually *slower*! `concurrent.futures` (and Dask) make it easy to swap the compute backend between threads and processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Parallelize `fib` with Processes\n",
    "\n",
    "Use a `concurrent.futures.ProcessPoolExecutor` to achieve the same task. Time how long it take.\n",
    "See https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor, which has the same API as our `ThreadPoolExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/04-schedulers-fib-process.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why use threads at all, if they can't actually run Python code in parallel? Because much of the code you're running isn't Python code. The performance-sensitive parts of libraries like NumPy and Pandas are written in C or Cython. Wherever possible, these libraries release the GIL. The standard library does this in places too, like when you make an HTTP request.\n",
    "\n",
    "When the GIL isn't a concern (NumPy and pandas), we tend to prefer threads to avoid data serialization.\n",
    "To do an operation on a pandas DataFrame in multiple processes, the data has to be serialized (using e.g. pickle) in  the first process and deserialized in the second process. This takes time and memory. Threads don't have serialization overhead.\n",
    "\n",
    "David Beazley has some nice materials on the GIL: http://www.dabeaz.com/GIL/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedulers\n",
    "\n",
    "In the previous notebooks, we used `dask.delayed` and `dask.dataframe` to parallelize computations.\n",
    "These work by building a *task graph* instead of executing immediately.\n",
    "Each *task* represents some function to call on some data, and the full *graph* is the relationship between all the tasks.\n",
    "\n",
    "When we wanted the actual result, we called `compute`, which handed the task graph off to a *scheduler*.\n",
    "\n",
    "**Schedulers are responsible for running a task graph and producing a result**.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dask/dask-org/master/images/grid_search_schedule.gif)\n",
    "\n",
    "Dask includes two types of schedulers.\n",
    "\n",
    "First, there are the single machine schedulers that execute things in parallel using threads or processes (or synchronously for debugging). These are what we've used up until now.\n",
    "\n",
    "Second, there's the Distributed scheduler, which is newer and has more features than the single machine scheduler. We'll discuss the advanced features [in part 6](06-distributed-advanced.ipynb). For now we'll introduce the distributed scheduler. Despite the name, the distributed scheduler works just fine on a single machine. We'll often recommend using it instead of the single-machine scheduler for the richer diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Scheduler\n",
    "\n",
    "The `dask.distributed` system is composed of a single centralized scheduler and many worker processes. [Deploying](http://dask.pydata.org/en/latest/setup.html) a remote Dask cluster involves some additional effort. But doing things locally is just involves creating a `Client` object, which lets you interact with the \"cluster\" (local threads or processes on your machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Setup a local cluster.\n",
    "# By default this sets up 1 worker per core\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to click the `Dashboard` link to open up the diagnostics dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "largest_delay = df.DepDelay.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, creating a `Client` makes it the default scheduler. Any calls to `.compute` will use the cluster your `client` is attached to (See http://dask.pydata.org/en/latest/scheduling.html for how to specify which scheduler to use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time largest_delay.compute().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run the following computations while looking at the diagnostics page. In each case what is taking the most time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of flights\n",
    "_ = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-cancelled flights\n",
    "_ = len(df[~df.Cancelled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-cancelled flights per-airport\n",
    "_ = df[~df.Cancelled].groupby('Origin').Origin.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average departure delay from each airport?\n",
    "_ = df[~df.Cancelled].groupby('Origin').DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average departure delay per day-of-week\n",
    "_ = df.groupby(df.Date.dt.dayofweek).DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "As a note, the `dask.distributed` scheduler supports and expands upon the `concurrent.futures` API.\n",
    "This means you can take your `concurrent.futures`-compatible code and scale it to a cluster by using `dask.distributed` instead of `concurrent.futures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = client.map(fib, [34, 34, 34, 34])\n",
    "%time _ = list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Advanced Distributed](06-distributed-advanced.ipynb) notebook will discuss this (and how `dask.distributed` enhances the `concurrent.futures` API) in more detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
