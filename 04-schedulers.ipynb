{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "## Threads, Processes, and the Global Interpreter Lock\n",
    "\n",
    "There are two main ways to achieve single-machine parallelism in python: multiple threads or multiple processes.\n",
    "The best choice depends on the type of work you're doing and Python's GIL (global interpreter lock).\n",
    "\n",
    "The GIL is an implementation detail of CPython. It's a lock deep inside your python process that limits things so *only one thread in your process can be running python code at once*. Let's see an example, using `concurrent.futures` to compute the fibonnaci numbers in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def fib(n):\n",
    "    \"\"\"Compute the `n`th fibonnaci number.\n",
    "    \n",
    "    This is a deliberatly slow, CPU intensive implemenation.\n",
    "    \"\"\"\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fib(n - 2) + fib(n - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time fib(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One `fib(34)` takes my machine about 2 seconds. Computing it 4 time should then take about 8 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = map(fib, [34, 34, 34, 34])\n",
    "%time _ = list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute it in parallel. This is embarassingly parallel, so with 4 threads we *should* be back down to 2 seconds again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_pool = concurrent.futures.ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "results = thread_pool.map(fib, [34, 34, 34, 34])\n",
    "%time _ = list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's actually *slower*! `concurrent.futures` (and Dask) make it easy to swap the compute backend between threads and processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Parallelize `fib` with Processes\n",
    "\n",
    "Use a `concurrent.futures.ProcessPoolExecutor` to achieve the same task. Time how long it take.\n",
    "See https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor, which has the same API as our `ThreadPoolExecutor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load solutions/04-schedulers-fib-process.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why use threads at all, if they can't actually run Python code in parallel? Becuase much of the code you're running isn't Python code. The performance-sennsitive parts of libraries like NumPy and Pandas are written in C or Cython. Wherever possible, these libraries release the GIL. The standard library does this in places too, like when you make an HTTP request.\n",
    "\n",
    "When the GIL isn't a concern (NumPy and pandas), we tend to prefer threads to avoid data serialization.\n",
    "To do an operation on a pandas DataFrame in multiple processes, the data has to be serialized (using e.g. pickle) in  the first process and deserialized in the second process. This takes time and memory. Threads don't have serialization overhead.\n",
    "\n",
    "David Beazley has some nice materials on the GIL: http://www.dabeaz.com/GIL/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedulers\n",
    "\n",
    "In the previous notebooks, we used `dask.delayed` and `dask.dataframe` to parallelize computations.\n",
    "These work by building a *task graph* instead of executing immediately.\n",
    "Each *task* represents some function to call on some data, and the full *graph* is the relationship between all the tasks.\n",
    "\n",
    "When we wanted the actual result, we called `compute`, which handed the task graph off to a *scheduler*.\n",
    "\n",
    "**Schedulers are responsible for running a task graph and producing a result**.\n",
    "\n",
    "![](https://raw.githubusercontent.com/dask/dask-org/master/images/grid_search_schedule.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask includes several schedulers\n",
    "\n",
    "Scheduler | Parallelism | Use Case\n",
    "--------- | ----------- | ---------\n",
    "threaded  | Local thread pool | Numeric, GIL-releasing code\n",
    "multiprocessing | Local process pool | GIL holding code\n",
    "local           | Single main thread | Debugging\n",
    "distributed     | Multiple machines  | Large problems\n",
    "\n",
    "In this section we first talk about changing schedulers.  Then we use the `dask.distributed` scheduler in more depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Schedulers\n",
    "\n",
    "Dask separates computation description (task graphs) from execution (schedulers). This allows you to write code once, and run it locally or scale it out across a cluster.\n",
    "\n",
    "In each case we change the scheduler used in a few different ways:\n",
    "\n",
    "- By providing a `get=` keyword argument to `compute`:\n",
    "\n",
    "```python\n",
    "total.compute(get=dask.multiprocessing.get)\n",
    "# or \n",
    "dask.compute(a, b, get=dask.multiprocessing.get)\n",
    "```\n",
    "\n",
    "- Using `dask.set_options`:\n",
    "\n",
    "```python\n",
    "# Use multiprocessing in this block\n",
    "with dask.set_options(get=dask.multiprocessing.get):\n",
    "    total.compute()\n",
    "# Use multiprocessing globally\n",
    "dask.set_options(get=dask.multiprocessing.get)\n",
    "```\n",
    "\n",
    "Here we repeat a simple dataframe computation from the previous section using the different schedulers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask \n",
    "import dask.multiprocessing\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': object,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})\n",
    "\n",
    "# Maximum non-cancelled delay\n",
    "largest_delay = df[~df.Cancelled].DepDelay.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute()  # this uses threads by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute(get=dask.multiprocessing.get)  # this uses processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute(get=dask.get)  # This uses a single thread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the threaded and multiprocessing schedulers use the same number of workers as cores. You can change this using the `num_workers` keyword in the same way that you specified `get` above:\n",
    "\n",
    "```\n",
    "largest_delay.compute(get=dask.multiprocessing.get, num_workers=2)\n",
    "```\n",
    "\n",
    "To see how many cores you have on your computer, you can use `multiprocessing.cpu_count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Questions to Consider:\n",
    "\n",
    "- How much speedup is possible for this task (hint, look at the graph).\n",
    "- Given how many cores are on this machine, how much faster could the parallel schedulers be than the single-threaded scheduler.\n",
    "- How much faster was using threads over a single thread? Why does this differ from the optimal speedup?\n",
    "- Why is the multiprocessing scheduler so much slower here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In what cases would you want to use one scheduler over another?\n",
    "\n",
    "http://dask.pydata.org/en/latest/scheduler-choice.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling\n",
    "\n",
    "*You should skip this section if you are running low on time*.\n",
    "\n",
    "The synchronous scheduler is particularly valuable for debugging and profiling.  \n",
    "\n",
    "For example, the IPython `%%prun` magic gives us profiling information about which functions take up the most time in our computation.  Try this magic on the computation above with different schedulers.  How informative is this magic when running parallel code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -l 30 _ = largest_delay.compute(get=dask.threaded.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -l 30 _ = largest_delay.compute(get=dask.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To aid in profiling parallel execution, dask provides several [`diagnostics`](http://dask.pydata.org/en/latest/diagnostics.html) for measuring and visualizing performance. These are useful for seeing bottlenecks in the *parallel* computation, whereas the above `prun` is useful for seeing bottlenecks in individual *tasks*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.diagnostics import Profiler, ResourceProfiler, visualize\n",
    "\n",
    "with Profiler() as p, ResourceProfiler(0.25) as r:\n",
    "    largest_delay.compute()\n",
    "    \n",
    "visualize([r, p]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, we can see that while tasks are running concurrently, due to GIL effects we're only achieving parallelism during early parts of `pd.read_csv` (mostly the byte operations).\n",
    "\n",
    "\n",
    "*It should be noted that the `dask.diagnostics` module is only useful when profiling on a single machine. The `dask.distributed` scheduler has its own set of diagnostics..*\n",
    "\n",
    "See Jim's talk at SciPy 2017 for more: https://www.youtube.com/watch?v=JoK8V2eWFPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Scheduler\n",
    "\n",
    "The `dask.distributed` system is composed of a single centralized scheduler and several worker processes.  We can either set these up manually as command line processes or have Dask set them up for us from the notebook.  \n",
    "\n",
    "\n",
    "#### Automatically setup a local cluster\n",
    "\n",
    "Starting a single scheduler and worker on the local machine is a common case. Dask will set up a local cluster for you if you provide no scheduler address to `Client`:\n",
    "\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "```\n",
    "\n",
    "If you choose this approach then there is no need to set up a `dask-scheduler` or `dask-worker` process as described below.\n",
    "\n",
    "\n",
    "#### Other ways to setup a cluster\n",
    "\n",
    "You can find more information at the following documentation pages:\n",
    "\n",
    "- [Quickstart](http://distributed.readthedocs.io/en/latest/quickstart.html)\n",
    "- [Many Ways to Setup](http://dask.pydata.org/en/latest/setup.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a local cluster\n",
    "\n",
    "As mentioned above, the multiprocessing scheduler can be inefficient for complicated workflows. The distributed scheduler doesn't have this downside, and works fine locally. This makes it often a good replacement for the multiprocessing scheduler, even when working on a single machine.\n",
    "\n",
    "Here we startup a local cluster, and use it to repeat the same dataframe computation as done above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Setup a local cluster.\n",
    "# By default this sets up 1 worker per core\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to click the `Dashboard` link to open up the diagnostics dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute(get=client.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Questions to Consider\n",
    "\n",
    "- How does this compare to the optimal parallel speedup?\n",
    "- Why is this faster than the threaded scheduler?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client takes over by default\n",
    "\n",
    "Actually, we didn't need to add `get=client.get`.  The distributed scheduler takes over as the default scheduler for all collections when the Client is created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time _ = largest_delay.compute()  # This used to use threads by default, now it uses dask.distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Run the following computations while looking at the diagnostics page. In each case what is taking the most time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of flights\n",
    "_ = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-cancelled flights\n",
    "_ = len(df[~df.Cancelled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of non-cancelled flights per-airport\n",
    "_ = df[~df.Cancelled].groupby('Origin').Origin.count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average departure delay from each airport?\n",
    "_ = df[~df.Cancelled].groupby('Origin').DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average departure delay per day-of-week\n",
    "_ = df.groupby(df.Date.dt.dayofweek).DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New API\n",
    "\n",
    "The distributed scheduler is more sophisticated than the single machine schedulers.  It comes with more functions to manage data, computing in the background, and more.  The distributed scheduler also has entirely separate documentation\n",
    "\n",
    "-  http://distributed.readthedocs.io/en/latest/\n",
    "-  http://distributed.readthedocs.io/en/latest/api.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
